{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34af30",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "!pip install ipywidgets plotly -q\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers, Model\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Для интерактивных графиков\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Для виджетов\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets\n",
    "\n",
    "# Версии библиотек\n",
    "print(f\"TensorFlow: {tf.__version__}\")\n",
    "print(f\"Keras: {keras.__version__}\")\n",
    "\n",
    "# Загружаем Fashion-MNIST\n",
    "(x_train, y_train), (x_test, y_test) = keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "# Нормализация данных\n",
    "x_train = x_train.astype('float32') / 255.0\n",
    "x_test = x_test.astype('float32') / 255.0\n",
    "\n",
    "# Добавляем размерность канала\n",
    "x_train = np.expand_dims(x_train, -1)\n",
    "x_test = np.expand_dims(x_test, -1)\n",
    "\n",
    "print(f\"Размер обучающей выборки: {x_train.shape}\")\n",
    "print(f\"Размер тестовой выборки: {x_test.shape}\")\n",
    "\n",
    "# Разделение на train/validation\n",
    "val_split = 0.1\n",
    "val_size = int(len(x_train) * val_split)\n",
    "x_val = x_train[:val_size]\n",
    "y_val = y_train[:val_size]\n",
    "x_train = x_train[val_size:]\n",
    "y_train = y_train[val_size:]\n",
    "\n",
    "fig, axes = plt.subplots(2, 5, figsize=(12, 5))\n",
    "axes = axes.ravel()\n",
    "\n",
    "class_names = ['T-shirt/top', 'Trouser', 'Pullover', 'Dress', 'Coat',\n",
    "               'Sandal', 'Shirt', 'Sneaker', 'Bag', 'Ankle boot']\n",
    "\n",
    "for i in range(10):\n",
    "    # Найдем первый пример каждого класса\n",
    "    idx = np.where(y_train == i)[0][0]\n",
    "    axes[i].imshow(x_train[idx].squeeze(), cmap='gray')\n",
    "    axes[i].set_title(f'{class_names[i]}')\n",
    "    axes[i].axis('off')\n",
    "\n",
    "plt.suptitle('Примеры изображений Fashion-MNIST', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "stats_df = pd.DataFrame({\n",
    "    'Класс': class_names,\n",
    "    'Количество в обучающей': [np.sum(y_train == i) for i in range(10)],\n",
    "    'Количество в тестовой': [np.sum(y_test == i) for i in range(10)]\n",
    "})\n",
    "\n",
    "print(\"Распределение по классам:\")\n",
    "print(stats_df)\n",
    "\n",
    "input_shape = (28, 28, 1)\n",
    "latent_dim = 32  # Размер латентного пространства\n",
    "\n",
    "def build_basic_autoencoder(latent_dim=32):\n",
    "    \"\"\"Создание базового автокодировщика\"\"\"\n",
    "\n",
    "    # Энкодер\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(encoder_inputs)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "    latent = layers.Dense(latent_dim, activation='relu', name='latent')(x)\n",
    "\n",
    "    encoder = Model(encoder_inputs, latent, name='encoder')\n",
    "\n",
    "    # Декодер\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "    x = layers.Dense(256, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(7 * 7 * 128, activation='relu')(x)\n",
    "    x = layers.Reshape((7, 7, 128))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(32, 3, activation='relu', padding='same')(x)\n",
    "    decoder_outputs = layers.Conv2D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, decoder_outputs, name='decoder')\n",
    "\n",
    "    # Автокодировщик\n",
    "    autoencoder_outputs = decoder(encoder(encoder_inputs))\n",
    "    autoencoder = Model(encoder_inputs, autoencoder_outputs, name='basic_autoencoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "def build_sparse_autoencoder(latent_dim=32, sparsity_weight=0.001):\n",
    "    \"\"\"Создание разреженного автокодировщика\"\"\"\n",
    "\n",
    "    # Энкодер с L1 регуляризацией\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(encoder_inputs)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.MaxPooling2D(2, padding='same')(x)\n",
    "\n",
    "    x = layers.Conv2D(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "\n",
    "    # Добавляем L1 Регуляризацию:\n",
    "    latent = layers.Dense(latent_dim, activation='relu',\n",
    "                         activity_regularizer=keras.regularizers.l1(sparsity_weight),\n",
    "                         name='latent')(x)\n",
    "\n",
    "    encoder = Model(encoder_inputs, latent, name='sparse_encoder')\n",
    "\n",
    "    # Декодер (такой же как в базовом)\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "    x = layers.Dense(256, activation='relu')(latent_inputs)\n",
    "    x = layers.Dense(7 * 7 * 128, activation='relu')(x)\n",
    "    x = layers.Reshape((7, 7, 128))(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(128, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(64, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation='relu', padding='same')(x)\n",
    "    x = layers.UpSampling2D(2)(x)\n",
    "\n",
    "    x = layers.Conv2DTranspose(32, 3, activation='relu', padding='same')(x)\n",
    "    decoder_outputs = layers.Conv2D(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, decoder_outputs, name='sparse_decoder')\n",
    "\n",
    "    # Автокодировщик\n",
    "    autoencoder_outputs = decoder(encoder(encoder_inputs))\n",
    "    autoencoder = Model(encoder_inputs, autoencoder_outputs, name='sparse_autoencoder')\n",
    "\n",
    "    return autoencoder, encoder, decoder\n",
    "\n",
    "class Sampling(layers.Layer):\n",
    "    \"\"\"Слой для семплирования из латентного распределения\"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        batch = tf.shape(z_mean)[0]\n",
    "        dim = tf.shape(z_mean)[1]\n",
    "        epsilon = tf.keras.backend.random_normal(shape=(batch, dim))\n",
    "        return z_mean + tf.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "def build_vae(latent_dim=32):\n",
    "    \"\"\"Создание вариационного автокодировщика\"\"\"\n",
    "\n",
    "    # Энкодер VAE\n",
    "    encoder_inputs = keras.Input(shape=input_shape)\n",
    "    x = layers.Conv2D(32, 3, activation='relu', strides=2, padding='same')(encoder_inputs)\n",
    "    x = layers.Conv2D(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(256, activation='relu')(x)\n",
    "\n",
    "    z_mean = layers.Dense(latent_dim, name='z_mean')(x)\n",
    "    z_log_var = layers.Dense(latent_dim, name='z_log_var')(x)\n",
    "    z = Sampling()([z_mean, z_log_var])\n",
    "\n",
    "    encoder = Model(encoder_inputs, [z_mean, z_log_var, z], name='vae_encoder')\n",
    "\n",
    "    # Декодер VAE\n",
    "    latent_inputs = keras.Input(shape=(latent_dim,))\n",
    "    x = layers.Dense(7 * 7 * 64, activation='relu')(latent_inputs)\n",
    "    x = layers.Reshape((7, 7, 64))(x)\n",
    "    x = layers.Conv2DTranspose(64, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    x = layers.Conv2DTranspose(32, 3, activation='relu', strides=2, padding='same')(x)\n",
    "    decoder_outputs = layers.Conv2DTranspose(1, 3, activation='sigmoid', padding='same')(x)\n",
    "\n",
    "    decoder = Model(latent_inputs, decoder_outputs, name='vae_decoder')\n",
    "\n",
    "    # Создаем VAE модель\n",
    "    vae_inputs = encoder_inputs\n",
    "    vae_outputs = decoder(encoder(vae_inputs)[2])\n",
    "\n",
    "    vae = Model(vae_inputs, vae_outputs, name='vae')\n",
    "\n",
    "    return vae, encoder, decoder\n",
    "\n",
    "def vae_loss(x, x_recon):\n",
    "    \"\"\"Функция потерь для VAE\"\"\"\n",
    "    # Reconstruction loss\n",
    "    reconstruction_loss = keras.losses.binary_crossentropy(x, x_recon)\n",
    "    reconstruction_loss = tf.reduce_mean(reconstruction_loss) * 784\n",
    "\n",
    "    # KL divergence loss\n",
    "    return reconstruction_loss\n",
    "\n",
    "# Дополнительная функция для вычисления KL потерь\n",
    "def kl_loss(z_mean, z_log_var):\n",
    "    \"\"\"Вычисление KL divergence\"\"\"\n",
    "    return -0.5 * tf.reduce_mean(\n",
    "        1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "    )\n",
    "\n",
    "print(\"Создание моделей...\")\n",
    "basic_ae, basic_encoder, basic_decoder = build_basic_autoencoder(latent_dim)\n",
    "sparse_ae, sparse_encoder, sparse_decoder = build_sparse_autoencoder(latent_dim)\n",
    "vae, vae_encoder, vae_decoder = build_vae(latent_dim)\n",
    "\n",
    "print(\"\\nАрхитектуры моделей:\")\n",
    "print(f\"Базовый автокодировщик: {basic_ae.count_params()} параметров\")\n",
    "print(f\"Разреженный автокодировщик: {sparse_ae.count_params()} параметров\")\n",
    "print(f\"Вариационный автокодировщик: {vae.count_params()} параметров\")\n",
    "\n",
    "# Базовый автокодировщик\n",
    "basic_ae.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Разреженный автокодировщик\n",
    "sparse_ae.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='mse',\n",
    "    metrics=['mae']\n",
    ")\n",
    "\n",
    "# Создаем VAE модель\n",
    "class CustomVAE(keras.Model):\n",
    "    def __init__(self, encoder, decoder, **kwargs):\n",
    "        super(CustomVAE, self).__init__(**kwargs)\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.total_loss_tracker = keras.metrics.Mean(name=\"total_loss\")\n",
    "        self.reconstruction_loss_tracker = keras.metrics.Mean(name=\"reconstruction_loss\")\n",
    "        self.kl_loss_tracker = keras.metrics.Mean(name=\"kl_loss\")\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.total_loss_tracker,\n",
    "            self.reconstruction_loss_tracker,\n",
    "            self.kl_loss_tracker,\n",
    "        ]\n",
    "\n",
    "    def train_step(self, data):\n",
    "        if isinstance(data, tuple):\n",
    "            x, _ = data\n",
    "        else:\n",
    "            x = data\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            z_mean, z_log_var, z = self.encoder(x)\n",
    "            reconstruction = self.decoder(z)\n",
    "\n",
    "            reconstruction_loss = keras.losses.binary_crossentropy(x, reconstruction)\n",
    "            reconstruction_loss = tf.reduce_mean(reconstruction_loss) * 784\n",
    "\n",
    "            kl_loss = -0.5 * tf.reduce_mean(\n",
    "                1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var)\n",
    "            )\n",
    "\n",
    "            total_loss = reconstruction_loss + kl_loss\n",
    "\n",
    "        grads = tape.gradient(total_loss, self.trainable_weights)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        self.total_loss_tracker.update_state(total_loss)\n",
    "        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n",
    "        self.kl_loss_tracker.update_state(kl_loss)\n",
    "\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "\n",
    "    def call(self, inputs):\n",
    "        _, _, z = self.encoder(inputs)\n",
    "        return self.decoder(z)\n",
    "\n",
    "\n",
    "vae_custom = CustomVAE(vae_encoder, vae_decoder)\n",
    "vae_custom.compile(optimizer=keras.optimizers.Adam(learning_rate=0.001))\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 50\n",
    "\n",
    "print(\"Обучение базового автокодировщика...\")\n",
    "basic_history = basic_ae.fit(\n",
    "    x_train, x_train,\n",
    "    validation_data=(x_val, x_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "sparse_history = sparse_ae.fit(\n",
    "    x_train, x_train,\n",
    "    validation_data=(x_val, x_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1,\n",
    "    callbacks=[\n",
    "        keras.callbacks.EarlyStopping(\n",
    "            monitor='val_loss',\n",
    "            patience=3,\n",
    "            restore_best_weights=True\n",
    "        ),\n",
    "        keras.callbacks.ReduceLROnPlateau(\n",
    "            monitor='val_loss',\n",
    "            factor=0.5,\n",
    "            patience=2,\n",
    "            min_lr=1e-6\n",
    "        )\n",
    "    ]\n",
    ")\n",
    "\n",
    "vae.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='binary_crossentropy'\n",
    ")\n",
    "\n",
    "vae_history = vae.fit(\n",
    "    x_train, x_train,\n",
    "    validation_data=(x_val, x_val),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(18, 4))\n",
    "\n",
    "# Потери базового автокодировщика\n",
    "axes[0].plot(basic_history.history['loss'], label='Обучающая', linewidth=2)\n",
    "axes[0].plot(basic_history.history['val_loss'], label='Валидационная', linewidth=2)\n",
    "axes[0].set_title('Базовый автокодировщик', fontsize=12, fontweight='bold')\n",
    "axes[0].set_xlabel('Эпоха')\n",
    "axes[0].set_ylabel('Потери (MSE)')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].fill_between(range(len(basic_history.history['loss'])),\n",
    "                     basic_history.history['loss'],\n",
    "                     basic_history.history['val_loss'],\n",
    "                     alpha=0.2)\n",
    "\n",
    "# Потери разреженного автокодировщика\n",
    "axes[1].plot(sparse_history.history['loss'], label='Обучающая', linewidth=2)\n",
    "axes[1].plot(sparse_history.history['val_loss'], label='Валидационная', linewidth=2)\n",
    "axes[1].set_title('Разреженный автокодировщик', fontsize=12, fontweight='bold')\n",
    "axes[1].set_xlabel('Эпоха')\n",
    "axes[1].set_ylabel('Потери (MSE)')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].fill_between(range(len(sparse_history.history['loss'])),\n",
    "                     sparse_history.history['loss'],\n",
    "                     sparse_history.history['val_loss'],\n",
    "                     alpha=0.2)\n",
    "\n",
    "# Потери VAE\n",
    "axes[2].plot(vae_history.history['loss'], label='Обучающая', linewidth=2)\n",
    "axes[2].plot(vae_history.history['val_loss'], label='Валидационная', linewidth=2)\n",
    "axes[2].set_title('Вариационный автокодировщик', fontsize=12, fontweight='bold')\n",
    "axes[2].set_xlabel('Эпоха')\n",
    "axes[2].set_ylabel('Потери (Binary CE)')\n",
    "axes[2].legend()\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "axes[2].fill_between(range(len(vae_history.history['loss'])),\n",
    "                     vae_history.history['loss'],\n",
    "                     vae_history.history['val_loss'],\n",
    "                     alpha=0.2)\n",
    "\n",
    "plt.suptitle('Кривые обучения разных архитектур автокодировщиков',\n",
    "             fontsize=14, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Примечание: VAE использует binary_crossentropy, другие - MSE\n",
    "# Для сравнения нужно конвертировать потери\n",
    "\n",
    "final_losses = pd.DataFrame({\n",
    "    'Модель': ['Базовый', 'Разреженный', 'VAE'],\n",
    "    'Обучающая потеря': [\n",
    "        basic_history.history['loss'][-1],\n",
    "        sparse_history.history['loss'][-1],\n",
    "        vae_history.history['loss'][-1]\n",
    "    ],\n",
    "    'Валидационная потеря': [\n",
    "        basic_history.history['val_loss'][-1],\n",
    "        sparse_history.history['val_loss'][-1],\n",
    "        vae_history.history['val_loss'][-1]\n",
    "    ],\n",
    "    'Тип потерь': ['MSE', 'MSE', 'Binary CE'],\n",
    "    'Количество эпох': [\n",
    "        len(basic_history.history['loss']),\n",
    "        len(sparse_history.history['loss']),\n",
    "        len(vae_history.history['loss'])\n",
    "    ]\n",
    "})\n",
    "\n",
    "print(\"\\nФинальные потери (разные метрики!):\")\n",
    "display(final_losses.style.format({\n",
    "    'Обучающая потеря': '{:.6f}',\n",
    "    'Валидационная потеря': '{:.6f}',\n",
    "    'Количество эпох': '{:d}'\n",
    "}).background_gradient(cmap='Reds_r', subset=['Обучающая потеря', 'Валидационная потеря']))\n",
    "\n",
    "# Создаем директорию для сохранения моделей\n",
    "os.makedirs('saved_models', exist_ok=True)\n",
    "\n",
    "# Сохраняем веса моделей\n",
    "basic_ae.save_weights('saved_models/basic_autoencoder.weights.h5')\n",
    "sparse_ae.save_weights('saved_models/sparse_autoencoder.weights.h5')\n",
    "vae.save_weights('saved_models/vae.weights.h5')\n",
    "\n",
    "# Также сохраняем полные модели\n",
    "basic_ae.save('saved_models/basic_autoencoder.keras')\n",
    "sparse_ae.save('saved_models/sparse_autoencoder.keras')\n",
    "vae.save('saved_models/vae.keras')\n",
    "\n",
    "print(\"Модели сохранены в директории 'saved_models/':\")\n",
    "print(\"\\nВ формате .weights.h5:\")\n",
    "print(\"1. basic_autoencoder.weights.h5\")\n",
    "print(\"2. sparse_autoencoder.weights.h5\")\n",
    "print(\"3. vae.weights.h5\")\n",
    "print(\"\\nВ формате .keras:\")\n",
    "print(\"1. basic_autoencoder.keras\")\n",
    "print(\"2. sparse_autoencoder.keras\")\n",
    "print(\"3. vae.keras\")\n",
    "\n",
    "# Добавим вывод информации о классах\n",
    "def compare_reconstructions_with_labels(models, model_names, n_images=5):\n",
    "    \"\"\"Сравнение реконструкций с выводом информации о классах\"\"\"\n",
    "\n",
    "    # Выбираем случайные изображения разных классов\n",
    "    indices = []\n",
    "    selected_classes = []\n",
    "    for class_id in range(10):\n",
    "        class_indices = np.where(y_test == class_id)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            idx = np.random.choice(class_indices)\n",
    "            indices.append(idx)\n",
    "            selected_classes.append(class_names[class_id])\n",
    "        if len(indices) >= n_images:\n",
    "            break\n",
    "\n",
    "    # Если не нашли достаточно разных классов, добавляем случайные\n",
    "    while len(indices) < n_images:\n",
    "        idx = np.random.choice(len(x_test))\n",
    "        indices.append(idx)\n",
    "        selected_classes.append(class_names[y_test[idx]])\n",
    "\n",
    "    indices = indices[:n_images]\n",
    "    test_images = x_test[indices]\n",
    "\n",
    "    print(f\"\\nВыбранные изображения и их классы:\")\n",
    "    for i, (idx, cls) in enumerate(zip(indices, selected_classes)):\n",
    "        print(f\"  {i+1}. Индекс {idx}: {cls}\")\n",
    "\n",
    "    fig, axes = plt.subplots(len(models) + 1, n_images, figsize=(15, 10))\n",
    "\n",
    "    # Оригинальные изображения\n",
    "    for i, (idx, cls) in enumerate(zip(indices, selected_classes)):\n",
    "        axes[0, i].imshow(test_images[i].squeeze(), cmap='gray')\n",
    "        axes[0, i].set_title(f'{cls}', fontsize=10)\n",
    "        axes[0, i].axis('off')\n",
    "        if i == n_images // 2:\n",
    "            axes[0, i].set_title('Оригиналы', fontsize=12, fontweight='bold')\n",
    "\n",
    "    # Реконструкции для каждой модели\n",
    "    for model_idx, (model, name) in enumerate(zip(models, model_names), 1):\n",
    "        reconstructions = model.predict(test_images, verbose=0)\n",
    "\n",
    "        for i, (idx, cls) in enumerate(zip(indices, selected_classes)):\n",
    "            axes[model_idx, i].imshow(reconstructions[i].squeeze(), cmap='gray')\n",
    "            axes[model_idx, i].axis('off')\n",
    "\n",
    "            # Рассчитываем MSE для этого изображения\n",
    "            mse = np.mean((test_images[i] - reconstructions[i]) ** 2)\n",
    "            psnr = 20 * np.log10(1.0 / np.sqrt(mse)) if mse > 0 else float('inf')\n",
    "            axes[model_idx, i].set_title(f'MSE: {mse:.4f}', fontsize=9)\n",
    "\n",
    "            if i == n_images // 2:\n",
    "                axes[model_idx, i].set_ylabel(name, fontsize=11, fontweight='bold')\n",
    "\n",
    "    plt.suptitle('Сравнение реконструкций разных автокодировщиков',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Анализ качества по классам\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"АНАЛИЗ КАЧЕСТВА РЕКОНСТРУКЦИИ ПО КЛАССАМ:\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    for model, name in zip(models, model_names):\n",
    "        print(f\"\\n{name}:\")\n",
    "        reconstructions = model.predict(test_images, verbose=0)\n",
    "        for i, (idx, cls) in enumerate(zip(indices, selected_classes)):\n",
    "            mse = np.mean((test_images[i] - reconstructions[i]) ** 2)\n",
    "            print(f\"  • {cls}: MSE = {mse:.6f}\")\n",
    "\n",
    "# Запускаем с выводом классов\n",
    "compare_reconstructions_with_labels(\n",
    "    [basic_ae, sparse_ae, vae],\n",
    "    ['Базовый', 'Разреженный', 'VAE'],\n",
    "    n_images=5\n",
    ")\n",
    "\n",
    "def analyze_reconstruction_errors(n_samples=100):\n",
    "    \"\"\"Детальный анализ ошибок реконструкции\"\"\"\n",
    "\n",
    "    # Выбираем случайные изображения\n",
    "    indices = np.random.choice(len(x_test), n_samples, replace=False)\n",
    "    test_images = x_test[indices]\n",
    "\n",
    "    # Получаем реконструкции\n",
    "    basic_recon = basic_ae.predict(test_images, verbose=0)\n",
    "    sparse_recon = sparse_ae.predict(test_images, verbose=0)\n",
    "    vae_recon = vae.predict(test_images, verbose=0)\n",
    "\n",
    "    # Вычисляем ошибки\n",
    "    basic_errors = np.abs(test_images - basic_recon)\n",
    "    sparse_errors = np.abs(test_images - sparse_recon)\n",
    "    vae_errors = np.abs(test_images - vae_recon)\n",
    "\n",
    "    # Визуализация\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "\n",
    "    # Распределение ошибок\n",
    "    axes[0, 0].hist(basic_errors.flatten(), bins=50, alpha=0.7, label='Базовый', density=True)\n",
    "    axes[0, 0].hist(sparse_errors.flatten(), bins=50, alpha=0.7, label='Разреженный', density=True)\n",
    "    axes[0, 0].hist(vae_errors.flatten(), bins=50, alpha=0.7, label='VAE', density=True)\n",
    "    axes[0, 0].set_xlabel('Абсолютная ошибка')\n",
    "    axes[0, 0].set_ylabel('Плотность')\n",
    "    axes[0, 0].set_title('Распределение ошибок реконструкции')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Кумулятивное распределение ошибок\n",
    "    axes[0, 1].hist(basic_errors.flatten(), bins=50, alpha=0.7, label='Базовый',\n",
    "                   cumulative=True, density=True, histtype='step', linewidth=2)\n",
    "    axes[0, 1].hist(sparse_errors.flatten(), bins=50, alpha=0.7, label='Разреженный',\n",
    "                   cumulative=True, density=True, histtype='step', linewidth=2)\n",
    "    axes[0, 1].hist(vae_errors.flatten(), bins=50, alpha=0.7, label='VAE',\n",
    "                   cumulative=True, density=True, histtype='step', linewidth=2)\n",
    "    axes[0, 1].set_xlabel('Абсолютная ошибка')\n",
    "    axes[0, 1].set_ylabel('Кумулятивная вероятность')\n",
    "    axes[0, 1].set_title('Кумулятивное распределение ошибок')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Статистика ошибок по моделям\n",
    "    error_stats = pd.DataFrame({\n",
    "        'Модель': ['Базовый', 'Разреженный', 'VAE'],\n",
    "        'Средняя ошибка': [\n",
    "            np.mean(basic_errors),\n",
    "            np.mean(sparse_errors),\n",
    "            np.mean(vae_errors)\n",
    "        ],\n",
    "        'Максимальная ошибка': [\n",
    "            np.max(basic_errors),\n",
    "            np.max(sparse_errors),\n",
    "            np.max(vae_errors)\n",
    "        ],\n",
    "        'Медианная ошибка': [\n",
    "            np.median(basic_errors),\n",
    "            np.median(sparse_errors),\n",
    "            np.median(vae_errors)\n",
    "        ],\n",
    "        '95-й перцентиль': [\n",
    "            np.percentile(basic_errors, 95),\n",
    "            np.percentile(sparse_errors, 95),\n",
    "            np.percentile(vae_errors, 95)\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    # Тепловая карта средних ошибок по классам\n",
    "    class_errors = np.zeros((3, 10))  # 3 модели × 10 классов\n",
    "\n",
    "    for class_id in range(10):\n",
    "        class_indices = np.where(y_test[indices] == class_id)[0]\n",
    "        if len(class_indices) > 0:\n",
    "            class_imgs = test_images[class_indices]\n",
    "\n",
    "            basic_class_recon = basic_ae.predict(class_imgs, verbose=0)\n",
    "            sparse_class_recon = sparse_ae.predict(class_imgs, verbose=0)\n",
    "            vae_class_recon = vae.predict(class_imgs, verbose=0)\n",
    "\n",
    "            class_errors[0, class_id] = np.mean(np.abs(class_imgs - basic_class_recon))\n",
    "            class_errors[1, class_id] = np.mean(np.abs(class_imgs - sparse_class_recon))\n",
    "            class_errors[2, class_id] = np.mean(np.abs(class_imgs - vae_class_recon))\n",
    "\n",
    "    im = axes[0, 2].imshow(class_errors, cmap='YlOrRd', aspect='auto')\n",
    "    axes[0, 2].set_xticks(range(10))\n",
    "    axes[0, 2].set_xticklabels([c[:3] for c in class_names], rotation=45)\n",
    "    axes[0, 2].set_yticks(range(3))\n",
    "    axes[0, 2].set_yticklabels(['Базовый', 'Разреженный', 'VAE'])\n",
    "    axes[0, 2].set_title('Средняя ошибка по классам')\n",
    "    plt.colorbar(im, ax=axes[0, 2])\n",
    "\n",
    "    # Добавляем значения в ячейки\n",
    "    for i in range(3):\n",
    "        for j in range(10):\n",
    "            axes[0, 2].text(j, i, f'{class_errors[i, j]:.3f}',\n",
    "                           ha='center', va='center', color='black', fontsize=8)\n",
    "\n",
    "    # Boxplot ошибок\n",
    "    error_data = [basic_errors.flatten(), sparse_errors.flatten(), vae_errors.flatten()]\n",
    "    axes[1, 0].boxplot(error_data, labels=['Базовый', 'Разреженный', 'VAE'])\n",
    "    axes[1, 0].set_ylabel('Абсолютная ошибка')\n",
    "    axes[1, 0].set_title('Boxplot ошибок реконструкции')\n",
    "    axes[1, 0].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # Scatter plot: MSE vs сложность изображения\n",
    "    # Сложность = стандартное отклонение пикселей\n",
    "    image_complexity = np.std(test_images, axis=(1, 2, 3))\n",
    "    basic_mse_per_image = np.mean((test_images - basic_recon) ** 2, axis=(1, 2, 3))\n",
    "    sparse_mse_per_image = np.mean((test_images - sparse_recon) ** 2, axis=(1, 2, 3))\n",
    "    vae_mse_per_image = np.mean((test_images - vae_recon) ** 2, axis=(1, 2, 3))\n",
    "\n",
    "    axes[1, 1].scatter(image_complexity, basic_mse_per_image, alpha=0.5, label='Базовый', s=20)\n",
    "    axes[1, 1].scatter(image_complexity, sparse_mse_per_image, alpha=0.5, label='Разреженный', s=20)\n",
    "    axes[1, 1].scatter(image_complexity, vae_mse_per_image, alpha=0.5, label='VAE', s=20)\n",
    "    axes[1, 1].set_xlabel('Сложность изображения (std)')\n",
    "    axes[1, 1].set_ylabel('MSE')\n",
    "    axes[1, 1].set_title('Зависимость MSE от сложности изображения')\n",
    "    axes[1, 1].legend()\n",
    "    axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "    # Пустая ось для статистики\n",
    "    axes[1, 2].axis('off')\n",
    "    stats_text = \"Статистика ошибок:\\n\\n\"\n",
    "    for idx, row in error_stats.iterrows():\n",
    "        stats_text += f\"{row['Модель']}:\\n\"\n",
    "        stats_text += f\"  Среднее: {row['Средняя ошибка']:.4f}\\n\"\n",
    "        stats_text += f\"  Медиана: {row['Медианная ошибка']:.4f}\\n\"\n",
    "        stats_text += f\"  95%: {row['95-й перцентиль']:.4f}\\n\"\n",
    "        stats_text += f\"  Макс: {row['Максимальная ошибка']:.4f}\\n\\n\"\n",
    "\n",
    "    axes[1, 2].text(0.1, 0.5, stats_text, fontsize=10,\n",
    "                   verticalalignment='center', fontfamily='monospace')\n",
    "\n",
    "    plt.suptitle('Детальный анализ ошибок реконструкции',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    return error_stats\n",
    "\n",
    "error_stats = analyze_reconstruction_errors(n_samples=200)\n",
    "\n",
    "# @title **6.3 Визуализация латентного пространства**\n",
    "\n",
    "def visualize_latent_space(encoder, x_data, y_data, title, is_vae=False, n_samples=500):\n",
    "    \"\"\"Визуализация латентного пространства\"\"\"\n",
    "\n",
    "    # Используем подвыборку для ускорения\n",
    "    x_subset = x_data[:n_samples]\n",
    "    y_subset = y_data[:n_samples]\n",
    "\n",
    "    print(f\"Визуализация латентного пространства для {title}...\")\n",
    "\n",
    "    try:\n",
    "        # Получаем латентные представления\n",
    "        if is_vae:\n",
    "            # Для VAE энкодер возвращает три значения\n",
    "            predictions = encoder.predict(x_subset, verbose=0)\n",
    "            if isinstance(predictions, list) and len(predictions) >= 3:\n",
    "                z_mean = predictions[0]\n",
    "                latent_vectors = z_mean\n",
    "            else:\n",
    "                latent_vectors = predictions\n",
    "        else:\n",
    "            latent_vectors = encoder.predict(x_subset, verbose=0)\n",
    "\n",
    "        print(f\"Размер латентных векторов: {latent_vectors.shape}\")\n",
    "\n",
    "        # Если размерность больше 2, берем первые 2 измерения\n",
    "        if latent_vectors.shape[1] > 2:\n",
    "            # Используем первые два латентных измерения\n",
    "            latent_2d = latent_vectors[:, :2]\n",
    "            dim_info = \" (первые 2 измерения)\"\n",
    "        else:\n",
    "            latent_2d = latent_vectors\n",
    "            dim_info = \"\"\n",
    "\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "        # 1. Проекция на первые 2 латентных измерения\n",
    "        scatter1 = axes[0, 0].scatter(latent_2d[:, 0], latent_2d[:, 1],\n",
    "                                     c=y_subset, cmap='tab10', alpha=0.7, s=30)\n",
    "        axes[0, 0].set_title(f'{title}\\nПервые 2 латентных измерения{dim_info}', fontsize=12)\n",
    "        axes[0, 0].set_xlabel('Латентное измерение 1')\n",
    "        axes[0, 0].set_ylabel('Латентное измерение 2')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        plt.colorbar(scatter1, ax=axes[0, 0], label='Класс')\n",
    "\n",
    "        # 2. Случайная проекция 2D (берем случайные 2 измерения)\n",
    "        if latent_vectors.shape[1] > 3:\n",
    "            # Выбираем случайные 2 измерения\n",
    "            import random\n",
    "            dim1, dim2 = random.sample(range(latent_vectors.shape[1]), 2)\n",
    "            scatter2 = axes[0, 1].scatter(latent_vectors[:, dim1], latent_vectors[:, dim2],\n",
    "                                         c=y_subset, cmap='tab10', alpha=0.7, s=30)\n",
    "            axes[0, 1].set_title(f'{title}\\nСлучайные измерения {dim1} и {dim2}', fontsize=12)\n",
    "            axes[0, 1].set_xlabel(f'Измерение {dim1}')\n",
    "            axes[0, 1].set_ylabel(f'Измерение {dim2}')\n",
    "            axes[0, 1].grid(True, alpha=0.3)\n",
    "            plt.colorbar(scatter2, ax=axes[0, 1], label='Класс')\n",
    "        else:\n",
    "            axes[0, 1].axis('off')\n",
    "            axes[0, 1].text(0.5, 0.5, 'Недостаточно измерений\\nдля случайной проекции',\n",
    "                           ha='center', va='center', fontsize=12)\n",
    "\n",
    "        # 3. Гистограмма активаций латентных нейронов\n",
    "        axes[0, 2].hist(latent_vectors.flatten(), bins=50, alpha=0.7, color='purple', density=True)\n",
    "        axes[0, 2].set_xlabel('Значение активации')\n",
    "        axes[0, 2].set_ylabel('Плотность')\n",
    "        axes[0, 2].set_title('Распределение всех латентных активаций')\n",
    "        axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "        # Добавляем статистику на график\n",
    "        mean_activation = np.mean(latent_vectors)\n",
    "        std_activation = np.std(latent_vectors)\n",
    "        axes[0, 2].axvline(mean_activation, color='red', linestyle='--',\n",
    "                          label=f'Среднее: {mean_activation:.3f}')\n",
    "        axes[0, 2].axvline(mean_activation + std_activation, color='orange', linestyle=':')\n",
    "        axes[0, 2].axvline(mean_activation - std_activation, color='orange', linestyle=':')\n",
    "        axes[0, 2].legend()\n",
    "\n",
    "        # 4. Heatmap латентных активаций (первые 50 примеров, все измерения)\n",
    "        n_examples = min(50, len(latent_vectors))\n",
    "        n_dims = min(32, latent_vectors.shape[1])  # Показываем первые 32 измерения\n",
    "\n",
    "        heatmap_data = latent_vectors[:n_examples, :n_dims]\n",
    "        im = axes[1, 0].imshow(heatmap_data, aspect='auto', cmap='viridis',\n",
    "                              interpolation='nearest')\n",
    "        axes[1, 0].set_xlabel('Латентное измерение')\n",
    "        axes[1, 0].set_ylabel('Пример')\n",
    "        axes[1, 0].set_title(f'Heatmap латентных активаций\\n(первые {n_examples} примеров, {n_dims} измерений)')\n",
    "        plt.colorbar(im, ax=axes[1, 0])\n",
    "\n",
    "        # 5. Средние активации по измерениям\n",
    "        mean_per_dimension = np.mean(latent_vectors, axis=0)\n",
    "        std_per_dimension = np.std(latent_vectors, axis=0)\n",
    "\n",
    "        x_pos = np.arange(len(mean_per_dimension))\n",
    "        axes[1, 1].bar(x_pos, mean_per_dimension, alpha=0.7, color='blue',\n",
    "                      yerr=std_per_dimension, error_kw={'elinewidth': 1, 'capthick': 1})\n",
    "        axes[1, 1].set_xlabel('Латентное измерение')\n",
    "        axes[1, 1].set_ylabel('Средняя активация')\n",
    "        axes[1, 1].set_title('Средние активации по измерениям')\n",
    "        axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "        # 6. Статистика и выводы\n",
    "        axes[1, 2].axis('off')\n",
    "\n",
    "        # Вычисляем статистику\n",
    "        threshold = 0.01\n",
    "        sparsity = np.sum(np.abs(latent_vectors) < threshold) / latent_vectors.size * 100\n",
    "\n",
    "        stats_text = f\"СТАТИСТИКА ЛАТЕНТНОГО ПРОСТРАНСТВА:\\n\\n\"\n",
    "        stats_text += f\"Размерность: {latent_vectors.shape[1]}\\n\"\n",
    "        stats_text += f\"Количество примеров: {len(latent_vectors)}\\n\"\n",
    "        stats_text += f\"Средняя активация: {mean_activation:.4f}\\n\"\n",
    "        stats_text += f\"Стандартное отклонение: {std_activation:.4f}\\n\"\n",
    "        stats_text += f\"Минимальная: {np.min(latent_vectors):.4f}\\n\"\n",
    "        stats_text += f\"Максимальная: {np.max(latent_vectors):.4f}\\n\"\n",
    "        stats_text += f\"Разреженность (<0.01): {sparsity:.1f}%\\n\\n\"\n",
    "\n",
    "        # Особенности по типу модели\n",
    "        if \"Разреженный\" in title:\n",
    "            stats_text += \"ОЖИДАЕМЫЕ ОСОБЕННОСТИ:\\n\"\n",
    "            stats_text += \"Высокая разреженность\\n\"\n",
    "            stats_text += \"Много активаций ≈0\\n\"\n",
    "            stats_text += \"Немного сильных активаций\\n\"\n",
    "        elif \"VAE\" in title:\n",
    "            stats_text += \"ОЖИДАЕМЫЕ ОСОБЕННОСТИ:\\n\"\n",
    "            stats_text += \"Нормальное распределение\\n\"\n",
    "            stats_text += \"Среднее ≈0\\n\"\n",
    "            stats_text += \"Хорошая структурированность\\n\"\n",
    "        else:\n",
    "            stats_text += \"ОЖИДАЕМЫЕ ОСОБЕННОСТИ:\\n\"\n",
    "            stats_text += \"Равномерное распределение\\n\"\n",
    "            stats_text += \"Низкая разреженность\\n\"\n",
    "            stats_text += \"Меньше структуры\\n\"\n",
    "\n",
    "        axes[1, 2].text(0.05, 0.95, stats_text, fontsize=10,\n",
    "                       verticalalignment='top', fontfamily='monospace',\n",
    "                       bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "\n",
    "        plt.suptitle(f'Визуализация латентного пространства: {title}',\n",
    "                     fontsize=16, fontweight='bold', y=1.02)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "        # Выводим статистику в консоль\n",
    "        print(f\"Статистика латентного пространства для {title}:\")\n",
    "        print(f\"Среднее значение активаций: {mean_activation:.4f}\")\n",
    "        print(f\"Стандартное отклонение: {std_activation:.4f}\")\n",
    "        print(f\"Минимальная активация: {np.min(latent_vectors):.4f}\")\n",
    "        print(f\"Максимальная активация: {np.max(latent_vectors):.4f}\")\n",
    "        print(f\"Разреженность (<0.01): {sparsity:.1f}%\")\n",
    "\n",
    "        if \"Разреженный\" in title:\n",
    "            print(f\"Процент нулевых активаций (<0.001): {np.sum(np.abs(latent_vectors) < 0.001) / latent_vectors.size * 100:.1f}%\")\n",
    "            print(f\"Процент высоких активаций (>0.5): {np.sum(np.abs(latent_vectors) > 0.5) / latent_vectors.size * 100:.1f}%\")\n",
    "\n",
    "        return latent_vectors\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Ошибка при визуализации для {title}: {e}\")\n",
    "        return None\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"ВИЗУАЛИЗАЦИЯ ЛАТЕНТНЫХ ПРОСТРАНСТВ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "n_samples = 300\n",
    "\n",
    "# Визуализируем для каждой модели\n",
    "try:\n",
    "    print(f\"1. Базовый автокодировщик (используем {n_samples} сэмплов):\")\n",
    "    basic_latent = visualize_latent_space(basic_encoder, x_test, y_test,\n",
    "                                         'Базовый автокодировщик', n_samples=n_samples)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"2. Разреженный автокодировщик (используем {n_samples} сэмплов):\")\n",
    "    sparse_latent = visualize_latent_space(sparse_encoder, x_test, y_test,\n",
    "                                          'Разреженный автокодировщик', n_samples=n_samples)\n",
    "\n",
    "    print(\"\\n\" + \"-\" * 60)\n",
    "    print(f\"3. Вариационный автокодировщик (VAE) (используем {n_samples} сэмплов):\")\n",
    "    vae_latent = visualize_latent_space(vae_encoder, x_test, y_test,\n",
    "                                       'Вариационный автокодировщик', is_vae=True, n_samples=n_samples)\n",
    "\n",
    "except NameError as e:\n",
    "    print(f\"Ошибка: {e}\")\n",
    "    print(\"Модели не определены. Вот что мы ожидаем увидеть:\")\n",
    "\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ТЕОРЕТИЧЕСКИЙ АНАЛИЗ ЛАТЕНТНЫХ ПРОСТРАНСТВ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    print(\"1. БАЗОВЫЙ АВТОКОДИРОВЩИК:\")\n",
    "    print(\"   Распределение: равномерное или гауссовское\")\n",
    "    print(\"   Разреженность: низкая (10-30%)\")\n",
    "    print(\"   Структура: слабая кластеризация по классам\")\n",
    "    print(\"   Среднее: зависит от активации (ReLU → положительное)\")\n",
    "\n",
    "    print(\"\\n2. РАЗРЕЖЕННЫЙ АВТОКОДИРОВЩИК:\")\n",
    "    print(\"   Распределение: много активаций ≈0, несколько сильных\")\n",
    "    print(\"   Разреженность: высокая (70-90%)\")\n",
    "    print(\"   Структура: более компактное представление\")\n",
    "    print(\"   Среднее: близко к 0 из-за регуляризации\")\n",
    "\n",
    "    print(\"\\n3. VAE:\")\n",
    "    print(\"   Распределение: нормальное (гауссовское)\")\n",
    "    print(\"   Разреженность: средняя (20-50%)\")\n",
    "    print(\"   Структура: хорошо кластеризовано по классам\")\n",
    "    print(\"   Среднее: ≈0 (KL дивергенция к N(0,1))\")\n",
    "\n",
    "    basic_latent = sparse_latent = vae_latent = None\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"СРАВНЕНИЕ ЛАТЕНТНЫХ ПРОСТРАНСТВ\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Создаем сравнительную таблицу\n",
    "if basic_latent is not None or sparse_latent is not None:\n",
    "    print(\"СРАВНИТЕЛЬНЫЙ АНАЛИЗ ЛАТЕНТНЫХ ПРОСТРАНСТВ:\")\n",
    "\n",
    "    models_info = [\n",
    "        (\"Базовый\", \"Высокое качество реконструкции\\nНизкая разреженность\\nСлабая структурированность\"),\n",
    "        (\"Разреженный\", \"Среднее/низкое качество\\nВысокая разреженность\\nКомпактные представления\"),\n",
    "        (\"VAE\", \"Хорошее качество\\nНормальное распределение\\nХорошая кластеризация\")\n",
    "    ]\n",
    "\n",
    "    for name, info in models_info:\n",
    "        print(f\"\\n{name} автокодировщик:\")\n",
    "        print(info)\n",
    "\n",
    "    print(\"\\nВЫВОДЫ ДЛЯ ЗАДАЧИ СЖАТИЯ ДАННЫХ:\")\n",
    "    print(\"1. Базовый: лучший для точной реконструкции, но плохое сжатие\")\n",
    "    print(\"2. Разреженный: лучшее сжатие (много нулей), но потеря качества\")\n",
    "    print(\"3. VAE: баланс качества и структурированности\")\n",
    "\n",
    "print(\"\\nВизуализация латентных пространств завершена\")\n",
    "\n",
    "def latent_interpolation_demo():\n",
    "    \"\"\"Демонстрация интерполяции в латентном пространстве\"\"\"\n",
    "\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ДЕМОНСТРАЦИЯ ИНТЕРПОЛЯЦИИ В ЛАТЕНТНОМ ПРОСТРАНСТВЕ\")\n",
    "    print(\"=\" * 60)\n",
    "\n",
    "    # Выбираем интересные пары классов для интерполяции\n",
    "    interpolation_pairs = [\n",
    "        (0, 1, \"Футболка → Брюки\"),\n",
    "        (2, 4, \"Свитер → Пальто\"),\n",
    "        (5, 7, \"Сандалии → Кроссовки\"),\n",
    "        (6, 3, \"Рубашка → Платье\"),\n",
    "        (8, 9, \"Сумка → Ботильоны\")\n",
    "    ]\n",
    "\n",
    "    for class1, class2, description in interpolation_pairs:\n",
    "        print(f\"\\nИнтерполяция: {description}\")\n",
    "\n",
    "        # Находим примеры этих классов\n",
    "        class1_indices = np.where(y_test == class1)[0]\n",
    "        class2_indices = np.where(y_test == class2)[0]\n",
    "\n",
    "        if len(class1_indices) > 0 and len(class2_indices) > 0:\n",
    "            # Выбираем случайные примеры\n",
    "            idx1 = np.random.choice(class1_indices)\n",
    "            idx2 = np.random.choice(class2_indices)\n",
    "\n",
    "            img1 = x_test[idx1:idx1+1]\n",
    "            img2 = x_test[idx2:idx2+1]\n",
    "\n",
    "            # Создаем фигуру: 3 строки (модели) × (n_steps + 2) столбцов\n",
    "            n_steps = 8  # Уменьшим количество шагов для компактности\n",
    "            fig, axes = plt.subplots(3, n_steps + 2, figsize=(n_steps + 4, 6))\n",
    "\n",
    "            for model_idx, (encoder, decoder, model_name, is_vae) in enumerate([\n",
    "                (basic_encoder, basic_decoder, 'Базовый', False),\n",
    "                (sparse_encoder, sparse_decoder, 'Разреженный', False),\n",
    "                (vae_encoder, vae_decoder, 'VAE', True)\n",
    "            ]):\n",
    "                # Получаем латентные представления\n",
    "                if is_vae:\n",
    "                    z1_mean, _, _ = encoder.predict(img1, verbose=0)\n",
    "                    z2_mean, _, _ = encoder.predict(img2, verbose=0)\n",
    "                    z1, z2 = z1_mean, z2_mean\n",
    "                else:\n",
    "                    z1 = encoder.predict(img1, verbose=0)\n",
    "                    z2 = encoder.predict(img2, verbose=0)\n",
    "\n",
    "                # Линейная интерполяция\n",
    "                interpolated_imgs = []\n",
    "                alphas = np.linspace(0, 1, n_steps)\n",
    "\n",
    "                for alpha in alphas:\n",
    "                    z_interp = (1 - alpha) * z1 + alpha * z2\n",
    "                    recon = decoder.predict(z_interp, verbose=0)\n",
    "                    interpolated_imgs.append(recon[0])\n",
    "\n",
    "                # Визуализация - начало (индекс 0)\n",
    "                axes[model_idx, 0].imshow(img1.squeeze(), cmap='gray')\n",
    "                axes[model_idx, 0].set_title(f'{class_names[class1]}\\nНачало', fontsize=8)\n",
    "                axes[model_idx, 0].axis('off')\n",
    "\n",
    "                # Интерполяции (индексы 1 до n_steps)\n",
    "                for i in range(n_steps):\n",
    "                    axes[model_idx, i + 1].imshow(interpolated_imgs[i].squeeze(), cmap='gray')\n",
    "                    axes[model_idx, i + 1].axis('off')\n",
    "\n",
    "                    # Подписываем середину интерполяции\n",
    "                    if i == n_steps // 2 - 1:\n",
    "                        axes[model_idx, i + 1].set_title(f'{model_name}\\nα={alphas[i]:.1f}', fontsize=8)\n",
    "                    else:\n",
    "                        # Для остальных можно добавить значение alpha поменьше\n",
    "                        if i < n_steps // 2:\n",
    "                            axes[model_idx, i + 1].set_title(f'α={alphas[i]:.1f}', fontsize=7)\n",
    "\n",
    "                # Конец (последний индекс)\n",
    "                axes[model_idx, n_steps + 1].imshow(img2.squeeze(), cmap='gray')\n",
    "                axes[model_idx, n_steps + 1].set_title(f'{class_names[class2]}\\nКонец', fontsize=8)\n",
    "                axes[model_idx, n_steps + 1].axis('off')\n",
    "\n",
    "            plt.suptitle(f'Интерполяция: {description}', fontsize=14, fontweight='bold', y=1.02)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "            # Анализ интерполяции\n",
    "            print(f\"  Класс {class1} → Класс {class2}\")\n",
    "            print(f\"  Отображено для всех трех моделей\")\n",
    "\n",
    "            # Вычисляем и показываем MSE интерполяции\n",
    "            print(f\"  Статистика интерполяции:\")\n",
    "            for model_idx, (encoder, decoder, model_name, is_vae) in enumerate([\n",
    "                (basic_encoder, basic_decoder, 'Базовый', False),\n",
    "                (sparse_encoder, sparse_decoder, 'Разреженный', False),\n",
    "                (vae_encoder, vae_decoder, 'VAE', True)\n",
    "            ]):\n",
    "                if is_vae:\n",
    "                    z1_mean, _, _ = encoder.predict(img1, verbose=0)\n",
    "                    z2_mean, _, _ = encoder.predict(img2, verbose=0)\n",
    "                    z1, z2 = z1_mean, z2_mean\n",
    "                else:\n",
    "                    z1 = encoder.predict(img1, verbose=0)\n",
    "                    z2 = encoder.predict(img2, verbose=0)\n",
    "\n",
    "                # Евклидово расстояние между латентными векторами\n",
    "                latent_distance = np.linalg.norm(z1 - z2)\n",
    "\n",
    "                # Качество реконструкций начала и конца\n",
    "                recon1 = decoder.predict(z1, verbose=0)\n",
    "                recon2 = decoder.predict(z2, verbose=0)\n",
    "                mse1 = np.mean((img1 - recon1) ** 2)\n",
    "                mse2 = np.mean((img2 - recon2) ** 2)\n",
    "\n",
    "                print(f\"    {model_name}: расстояние={latent_distance:.2f}, MSE начала={mse1:.4f}, MSE конца={mse2:.4f}\")\n",
    "        else:\n",
    "            print(f\"  Не найдено примеров классов {class1} и {class2}\")\n",
    "\n",
    "# Запускаем демонстрацию интерполяции\n",
    "latent_interpolation_demo()\n",
    "\n",
    "def analyze_compression_efficiency():\n",
    "    \"\"\"Анализ эффективности сжатия\"\"\"\n",
    "\n",
    "    # Параметры сжатия\n",
    "    original_size = 28 * 28 * 8  # 28x28 пикселей × 8 бит на пиксель\n",
    "    latent_size = latent_dim * 32  # 32 латентных признака × 32 бита на признак\n",
    "\n",
    "    compression_ratio = original_size / latent_size\n",
    "    space_saving = (1 - latent_size / original_size) * 100\n",
    "\n",
    "    # Вычисляем качество для разных размеров батча\n",
    "    batch_sizes = [1, 10, 100, 1000]\n",
    "\n",
    "    results = []\n",
    "    for batch_size_test in batch_sizes:\n",
    "        test_subset = x_test[:batch_size_test]\n",
    "\n",
    "        # Время предсказания\n",
    "        import time\n",
    "\n",
    "        # Базовый автокодировщик\n",
    "        start = time.time()\n",
    "        basic_recon = basic_ae.predict(test_subset, verbose=0)\n",
    "        basic_time = time.time() - start\n",
    "\n",
    "        # Разреженный автокодировщик\n",
    "        start = time.time()\n",
    "        sparse_recon = sparse_ae.predict(test_subset, verbose=0)\n",
    "        sparse_time = time.time() - start\n",
    "\n",
    "        # VAE\n",
    "        start = time.time()\n",
    "        vae_recon = vae.predict(test_subset, verbose=0)\n",
    "        vae_time = time.time() - start\n",
    "\n",
    "        # Качество\n",
    "        basic_mse = np.mean((test_subset - basic_recon) ** 2)\n",
    "        sparse_mse = np.mean((test_subset - sparse_recon) ** 2)\n",
    "        vae_mse = np.mean((test_subset - vae_recon) ** 2)\n",
    "\n",
    "        # Сохраняем результаты с правильными ключами\n",
    "        results.append({\n",
    "            'batch_size': batch_size_test,\n",
    "            'basic_time': basic_time,\n",
    "            'sparse_time': sparse_time,\n",
    "            'vae_time': vae_time,\n",
    "            'basic_mse': basic_mse,\n",
    "            'sparse_mse': sparse_mse,\n",
    "            'vae_mse': vae_mse\n",
    "        })\n",
    "\n",
    "    # Визуализация\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "    # 1. Время обработки vs размер батча\n",
    "    batch_sizes = [r['batch_size'] for r in results]\n",
    "    basic_times = [r['basic_time'] for r in results]\n",
    "    sparse_times = [r['sparse_time'] for r in results]\n",
    "    vae_times = [r['vae_time'] for r in results]\n",
    "\n",
    "    axes[0, 0].plot(batch_sizes, basic_times, 'o-', label='Базовый', linewidth=2)\n",
    "    axes[0, 0].plot(batch_sizes, sparse_times, 's-', label='Разреженный', linewidth=2)\n",
    "    axes[0, 0].plot(batch_sizes, vae_times, '^-', label='VAE', linewidth=2)\n",
    "    axes[0, 0].set_xlabel('Размер батча')\n",
    "    axes[0, 0].set_ylabel('Время обработки (сек)')\n",
    "    axes[0, 0].set_title('Производительность при разных размерах батча')\n",
    "    axes[0, 0].legend()\n",
    "    axes[0, 0].grid(True, alpha=0.3)\n",
    "    axes[0, 0].set_xscale('log')\n",
    "    axes[0, 0].set_yscale('log')\n",
    "\n",
    "    # 2. Эффективность сжатия\n",
    "    compression_data = pd.DataFrame({\n",
    "        'Модель': ['Базовый', 'Разреженный', 'VAE'],\n",
    "        'Степень сжатия': [compression_ratio, compression_ratio, compression_ratio],\n",
    "        'Экономия памяти (%)': [space_saving, space_saving, space_saving],\n",
    "        'Среднее время на изображение (мс)': [\n",
    "            results[0]['basic_time'] * 1000,\n",
    "            results[0]['sparse_time'] * 1000,\n",
    "            results[0]['vae_time'] * 1000\n",
    "        ]\n",
    "    })\n",
    "\n",
    "    x = range(3)\n",
    "    width = 0.25\n",
    "\n",
    "    axes[0, 1].bar([i - width for i in x], compression_data['Степень сжатия'],\n",
    "                  width=width, label='Степень сжатия', alpha=0.7)\n",
    "    axes[0, 1].bar(x, compression_data['Экономия памяти (%)'],\n",
    "                  width=width, label='Экономия памяти (%)', alpha=0.7)\n",
    "    axes[0, 1].bar([i + width for i in x], compression_data['Среднее время на изображение (мс)'],\n",
    "                  width=width, label='Время (мс)', alpha=0.7)\n",
    "\n",
    "    axes[0, 1].set_xticks(x)\n",
    "    axes[0, 1].set_xticklabels(compression_data['Модель'])\n",
    "    axes[0, 1].set_ylabel('Значение')\n",
    "    axes[0, 1].set_title('Эффективность сжатия и производительность')\n",
    "    axes[0, 1].legend()\n",
    "    axes[0, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "    # 3. Trade-off: качество vs скорость\n",
    "    basic_speed = 1 / results[0]['basic_time'] if results[0]['basic_time'] > 0 else 0\n",
    "    sparse_speed = 1 / results[0]['sparse_time'] if results[0]['sparse_time'] > 0 else 0\n",
    "    vae_speed = 1 / results[0]['vae_time'] if results[0]['vae_time'] > 0 else 0\n",
    "\n",
    "    axes[1, 0].scatter([basic_speed], [results[0]['basic_mse']], s=200,\n",
    "                      label='Базовый', alpha=0.7)\n",
    "    axes[1, 0].scatter([sparse_speed], [results[0]['sparse_mse']], s=200,\n",
    "                      label='Разреженный', alpha=0.7)\n",
    "    axes[1, 0].scatter([vae_speed], [results[0]['vae_mse']], s=200,\n",
    "                      label='VAE', alpha=0.7)\n",
    "\n",
    "    axes[1, 0].set_xlabel('Скорость (изображений/сек)')\n",
    "    axes[1, 0].set_ylabel('MSE')\n",
    "    axes[1, 0].set_title('Компромисс: качество vs скорость')\n",
    "    axes[1, 0].legend()\n",
    "    axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "    # Добавляем аннотации\n",
    "    for model, speed, mse in zip(['Базовый', 'Разреженный', 'VAE'],\n",
    "                                [basic_speed, sparse_speed, vae_speed],\n",
    "                                [results[0]['basic_mse'], results[0]['sparse_mse'], results[0]['vae_mse']]):\n",
    "        axes[1, 0].annotate(model, (speed, mse), xytext=(5, 5),\n",
    "                          textcoords='offset points', fontsize=9)\n",
    "\n",
    "    # 4. Сводная таблица\n",
    "    axes[1, 1].axis('off')\n",
    "\n",
    "    summary_text = \"СВОДКА ЭФФЕКТИВНОСТИ СЖАТИЯ:\\n\\n\"\n",
    "    summary_text += f\"Исходный размер: {original_size} бит/изображение\\n\"\n",
    "    summary_text += f\"Сжатый размер: {latent_size} бит/изображение\\n\"\n",
    "    summary_text += f\"Степень сжатия: {compression_ratio:.1f}:1\\n\"\n",
    "    summary_text += f\"Экономия памяти: {space_saving:.1f}%\\n\\n\"\n",
    "\n",
    "    summary_text += \"СРАВНЕНИЕ МОДЕЛЕЙ:\\n\"\n",
    "    for model_name in ['Базовый', 'Разреженный', 'VAE']:\n",
    "        summary_text += f\"\\n{model_name}:\\n\"\n",
    "\n",
    "        # Получаем ключ для модели\n",
    "        if model_name == 'Базовый':\n",
    "            time_key = 'basic_time'\n",
    "            mse_key = 'basic_mse'\n",
    "        elif model_name == 'Разреженный':\n",
    "            time_key = 'sparse_time'\n",
    "            mse_key = 'sparse_mse'\n",
    "        else:  # VAE\n",
    "            time_key = 'vae_time'\n",
    "            mse_key = 'vae_mse'\n",
    "\n",
    "        # Используем результаты для batch_size=100 (индекс 2)\n",
    "        if len(results) > 2:\n",
    "            mse_value = results[2][mse_key]\n",
    "            time_value = results[0][time_key]  # Для одного изображения\n",
    "\n",
    "            summary_text += f\"  MSE: {mse_value:.6f}\\n\"\n",
    "            summary_text += f\"  Время (1 img): {time_value*1000:.1f} мс\\n\"\n",
    "            if time_value > 0:\n",
    "                summary_text += f\"  Скорость: {1/time_value:.1f} img/сек\\n\"\n",
    "            else:\n",
    "                summary_text += f\"  Скорость: 0 img/сек\\n\"\n",
    "        else:\n",
    "            summary_text += f\"  Данные недоступны\\n\"\n",
    "\n",
    "    axes[1, 1].text(0.1, 0.5, summary_text, fontsize=10,\n",
    "                   verticalalignment='center', fontfamily='monospace')\n",
    "\n",
    "    plt.suptitle('Анализ эффективности сжатия данных',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Ключевые показатели эффективности сжатия:\")\n",
    "    print(f\"Степень сжатия: {compression_ratio:.1f}:1\")\n",
    "    print(f\"Экономия памяти: {space_saving:.1f}%\")\n",
    "    print(f\"Базовый автокодировщик: {1/results[0]['basic_time']:.1f} изображений/сек\")\n",
    "    print(f\"Разреженный автокодировщик: {1/results[0]['sparse_time']:.1f} изображений/сек\")\n",
    "    print(f\"VAE: {1/results[0]['vae_time']:.1f} изображений/сек\")\n",
    "\n",
    "# Запускаем анализ\n",
    "analyze_compression_efficiency()\n",
    "\n",
    "def interactive_reconstruction(model_choice, image_idx):\n",
    "    \"\"\"Интерактивное сравнение оригинального и реконструированного изображения\"\"\"\n",
    "\n",
    "    models_dict = {\n",
    "        'Базовый автокодировщик': basic_ae,\n",
    "        'Разреженный автокодировщик': sparse_ae,\n",
    "        'Вариационный автокодировщик': vae\n",
    "    }\n",
    "\n",
    "    model = models_dict[model_choice]\n",
    "\n",
    "    # Выбираем изображение\n",
    "    test_image = x_test[image_idx:image_idx+1]\n",
    "    reconstruction = model.predict(test_image, verbose=0)[0]\n",
    "\n",
    "    # Вычисляем разницу\n",
    "    difference = np.abs(test_image[0] - reconstruction)\n",
    "\n",
    "    # Визуализация\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(15, 10))\n",
    "    axes = axes.ravel()\n",
    "\n",
    "    # Оригинал\n",
    "    axes[0].imshow(test_image[0].squeeze(), cmap='gray')\n",
    "    axes[0].set_title(f'Оригинал\\nКласс: {class_names[y_test[image_idx]]}',\n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[0].axis('off')\n",
    "\n",
    "    # Реконструкция\n",
    "    axes[1].imshow(reconstruction.squeeze(), cmap='gray')\n",
    "\n",
    "    # Вычисляем метрики\n",
    "    mse = np.mean((test_image[0] - reconstruction) ** 2)\n",
    "    psnr = 20 * np.log10(1.0 / np.sqrt(mse)) if mse > 0 else float('inf')\n",
    "\n",
    "    axes[1].set_title(f'{model_choice}\\nMSE: {mse:.6f}\\nPSNR: {psnr:.2f} dB',\n",
    "                      fontsize=12, fontweight='bold')\n",
    "    axes[1].axis('off')\n",
    "\n",
    "    # Разница (цветная)\n",
    "    im_diff = axes[2].imshow(difference.squeeze(), cmap='hot', vmin=0, vmax=1)\n",
    "    axes[2].set_title('Абсолютная разница', fontsize=12, fontweight='bold')\n",
    "    axes[2].axis('off')\n",
    "    plt.colorbar(im_diff, ax=axes[2], fraction=0.046, pad=0.04)\n",
    "\n",
    "    # Гистограмма различий\n",
    "    axes[3].hist(difference.flatten(), bins=50, alpha=0.7, color='blue',\n",
    "                edgecolor='black')\n",
    "    axes[3].set_title('Распределение различий', fontsize=12, fontweight='bold')\n",
    "    axes[3].set_xlabel('Величина различия')\n",
    "    axes[3].set_ylabel('Частота')\n",
    "    axes[3].grid(True, alpha=0.3)\n",
    "    axes[3].axvline(difference.mean(), color='red', linestyle='--',\n",
    "                   linewidth=2, label=f'Среднее: {difference.mean():.4f}')\n",
    "    axes[3].legend()\n",
    "\n",
    "    # Сравнение пиксель за пикселем\n",
    "    axes[4].scatter(test_image[0].flatten(), reconstruction.flatten(),\n",
    "                   alpha=0.3, s=1)\n",
    "    axes[4].plot([0, 1], [0, 1], 'r--', linewidth=2, label='Идеальная реконструкция')\n",
    "    axes[4].set_title('Сравнение пикселей', fontsize=12, fontweight='bold')\n",
    "    axes[4].set_xlabel('Оригинальные значения')\n",
    "    axes[4].set_ylabel('Реконструированные значения')\n",
    "    axes[4].legend()\n",
    "    axes[4].grid(True, alpha=0.3)\n",
    "\n",
    "    # Метрики качества\n",
    "    metrics_text = f\"\"\"\n",
    "    Метрики качества:\n",
    "    MSE: {mse:.6f}\n",
    "    PSNR: {psnr:.2f} dB\n",
    "    Макс. разница: {difference.max():.4f}\n",
    "    Сред. разница: {difference.mean():.4f}\n",
    "    Стандартное отклонение: {difference.std():.4f}\n",
    "    \"\"\"\n",
    "\n",
    "    axes[5].text(0.1, 0.5, metrics_text, fontsize=11,\n",
    "                verticalalignment='center', fontfamily='monospace')\n",
    "    axes[5].set_title('Статистика', fontsize=12, fontweight='bold')\n",
    "    axes[5].axis('off')\n",
    "\n",
    "    plt.suptitle(f'Детальный анализ реконструкции для изображения {image_idx}',\n",
    "                 fontsize=16, fontweight='bold', y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "print(\"\\nИспользуйте виджеты ниже для настройки:\")\n",
    "print(\"1. Выберите модель из выпадающего списка\")\n",
    "print(\"2. Измените номер изображения с помощью слайдера\")\n",
    "\n",
    "# Создаем интерактивные виджеты\n",
    "interact(\n",
    "    interactive_reconstruction,\n",
    "    model_choice=widgets.Dropdown(\n",
    "        options=['Базовый автокодировщик', 'Разреженный автокодировщик', 'Вариационный автокодировщик'],\n",
    "        value='Базовый автокодировщик',\n",
    "        description='Модель:',\n",
    "        style={'description_width': 'initial'}\n",
    "    ),\n",
    "    image_idx=widgets.IntSlider(\n",
    "        min=0,\n",
    "        max=min(1000, len(x_test)-1),  # Ограничиваем для производительности\n",
    "        value=0,\n",
    "        description='Номер изображения:',\n",
    "        continuous_update=False,\n",
    "        style={'description_width': 'initial'}\n",
    "    )\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
